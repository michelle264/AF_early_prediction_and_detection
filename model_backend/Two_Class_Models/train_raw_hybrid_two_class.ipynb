{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2e653d",
   "metadata": {},
   "source": [
    "### import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b66d9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_PATH = os.path.join(\"..\", \"Dataset\", \"raw_RRI_segments.csv\")\n",
    "OUT_MODEL = os.path.join(\"..\", \"Two_Class_Models\", \"saved_models\", \"raw_hybrid_two_class_best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b037d35",
   "metadata": {},
   "source": [
    "### Load Raw dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b232ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw samples: (9497, 50) class counts: [4750 4747]\n",
      "Unique labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Keep only classes 0 (SR) and 2 (AF)\n",
    "mask_binary = df[\"label\"].isin([0, 2])\n",
    "df = df[mask_binary].copy()\n",
    "\n",
    "# Remap labels: 0 → 0 (SR), 2 → 1 (AF)\n",
    "df[\"label\"] = df[\"label\"].map({0: 0, 2: 1})\n",
    "\n",
    "# Pick raw RRI columns (prefix 'r_')\n",
    "feature_cols = [c for c in df.columns if c.startswith(\"r_\")]\n",
    "meta_cols = [\"patient_id\", \"record_id\", \"label\", \"label_str\", \"orig_len\"]\n",
    "\n",
    "assert len(feature_cols) > 0, \"No r_ columns found in CSV\"\n",
    "\n",
    "# Extract feature matrix and labels\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "y = df[\"label\"].values.astype(int)\n",
    "\n",
    "# Drop rows with NaN or Inf\n",
    "mask_good = np.isfinite(X).all(axis=1)\n",
    "X = X[mask_good]\n",
    "y = y[mask_good]\n",
    "\n",
    "print(\"Loaded raw samples:\", X.shape, \"class counts:\", np.bincount(y))\n",
    "print(\"Unique labels:\", np.unique(y))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be49de3",
   "metadata": {},
   "source": [
    "### Train / val / test split, scaling and SMOTE (apply SMOTE only to training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7395a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits -> train: (6077, 50) val: (1520, 50) test: (1900, 50)\n",
      "After SMOTE train distribution: [3040 3040]\n"
     ]
    }
   ],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"Splits -> train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
    "\n",
    "# Standardize using training stats\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# SMOTE on training only\n",
    "smote = SMOTE(random_state=RANDOM_SEED)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE train distribution:\", np.bincount(y_train_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d412a",
   "metadata": {},
   "source": [
    "### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c4e425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# compute class weights from resampled training set (used as alpha for focal loss)\n",
    "cw = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_res), y=y_train_res)\n",
    "class_weights_tensor = torch.tensor(cw, dtype=torch.float32)\n",
    "print(\"Class weights:\", cw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94d44a",
   "metadata": {},
   "source": [
    "### Compute focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e2ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf97a5a",
   "metadata": {},
   "source": [
    "### Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2adfa881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model definitions (Hybrid NODE + Attention) ---\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, dim)\n",
    "        )\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(dim, dim)\n",
    "        self.key = nn.Linear(dim, dim)\n",
    "        self.value = nn.Linear(dim, dim)\n",
    "        self.scale = dim ** 0.5\n",
    "    def forward(self, x):\n",
    "        # x: [batch, dim] -> [batch, 1, dim]\n",
    "        x1 = x.unsqueeze(1)\n",
    "        Q = self.query(x1)\n",
    "        K = self.key(x1)\n",
    "        V = self.value(x1)\n",
    "        scores = torch.softmax(torch.bmm(Q, K.transpose(1,2)) / self.scale, dim=-1)\n",
    "        out = torch.bmm(scores, V)  # [batch,1,dim]\n",
    "        return out.squeeze(1)\n",
    "\n",
    "class HybridNODEAttentionModel(nn.Module):\n",
    "    def __init__(self, dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.odefunc = ODEFunc(dim)\n",
    "        self.attn = SelfAttention(dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: [batch, dim]\n",
    "        t = torch.tensor([0.0, 1.0], dtype=x.dtype, device=x.device)\n",
    "        ode_out = odeint(self.odefunc, x, t)[-1]\n",
    "        attn_out = self.attn(ode_out)\n",
    "        return self.classifier(attn_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa97a4a",
   "metadata": {},
   "source": [
    "### Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef3e112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_res.astype(np.float32)), torch.from_numpy(y_train_res.astype(np.int64)))\n",
    "val_ds = TensorDataset(torch.from_numpy(X_val.astype(np.float32)), torch.from_numpy(y_val.astype(np.int64)))\n",
    "test_ds = TensorDataset(torch.from_numpy(X_test.astype(np.float32)), torch.from_numpy(y_test.astype(np.int64)))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = X_train_res.shape[1]\n",
    "num_classes = int(np.unique(y).size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c8d37",
   "metadata": {},
   "source": [
    "### Train model (hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a48ed8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 8\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.0979 val_f1: 0.8392\n",
      "  epoch 2 train_loss: 0.0905 val_f1: 0.8354\n",
      "  epoch 3 train_loss: 0.0871 val_f1: 0.8651\n",
      "  epoch 4 train_loss: 0.0845 val_f1: 0.8651\n",
      "  epoch 5 train_loss: 0.0806 val_f1: 0.8484\n",
      "  early stopping\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 16, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1080 val_f1: 0.8379\n",
      "  epoch 2 train_loss: 0.0987 val_f1: 0.8400\n",
      "  epoch 3 train_loss: 0.0978 val_f1: 0.8400\n",
      "  epoch 4 train_loss: 0.0970 val_f1: 0.8353\n",
      "  epoch 5 train_loss: 0.0963 val_f1: 0.8359\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.0988 val_f1: 0.8311\n",
      "  epoch 2 train_loss: 0.0936 val_f1: 0.8612\n",
      "  epoch 3 train_loss: 0.0897 val_f1: 0.8612\n",
      "  epoch 4 train_loss: 0.0851 val_f1: 0.8664\n",
      "  epoch 5 train_loss: 0.0817 val_f1: 0.8664\n",
      "  epoch 6 train_loss: 0.0794 val_f1: 0.8631\n",
      "  epoch 7 train_loss: 0.0776 val_f1: 0.8664\n",
      "  early stopping\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1122 val_f1: 0.8341\n",
      "  epoch 2 train_loss: 0.1002 val_f1: 0.8321\n",
      "  epoch 3 train_loss: 0.0991 val_f1: 0.8321\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.0981 val_f1: 0.8490\n",
      "  epoch 2 train_loss: 0.0917 val_f1: 0.8605\n",
      "  epoch 3 train_loss: 0.0878 val_f1: 0.8592\n",
      "  epoch 4 train_loss: 0.0848 val_f1: 0.8539\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1181 val_f1: 0.8237\n",
      "  epoch 2 train_loss: 0.1014 val_f1: 0.8288\n",
      "  epoch 3 train_loss: 0.0998 val_f1: 0.8320\n",
      "  epoch 4 train_loss: 0.0987 val_f1: 0.8319\n",
      "  epoch 5 train_loss: 0.0980 val_f1: 0.8285\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.1011 val_f1: 0.8374\n",
      "  epoch 2 train_loss: 0.0949 val_f1: 0.8418\n",
      "  epoch 3 train_loss: 0.0910 val_f1: 0.8612\n",
      "  epoch 4 train_loss: 0.0877 val_f1: 0.8585\n",
      "  epoch 5 train_loss: 0.0852 val_f1: 0.8618\n",
      "  epoch 6 train_loss: 0.0829 val_f1: 0.8664\n",
      "  epoch 7 train_loss: 0.0803 val_f1: 0.8624\n",
      "  epoch 8 train_loss: 0.0785 val_f1: 0.8618\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1207 val_f1: 0.8242\n",
      "  epoch 2 train_loss: 0.1011 val_f1: 0.8283\n",
      "  epoch 3 train_loss: 0.0997 val_f1: 0.8335\n",
      "  epoch 4 train_loss: 0.0990 val_f1: 0.8335\n",
      "  epoch 5 train_loss: 0.0986 val_f1: 0.8295\n",
      "  early stopping\n",
      "\n",
      "Global best val F1: 0.8664 params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "Best params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'} Best val F1: 0.8664459232808527\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_with_hyperparameter_tuning(model_class, input_dim, num_classes,\n",
    "                                     train_dataset, val_dataset,\n",
    "                                     class_weights_tensor,\n",
    "                                     param_grid,\n",
    "                                     save_path=OUT_MODEL,\n",
    "                                     epochs=8,\n",
    "                                     patience=2,\n",
    "                                     device=None):\n",
    "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    class_weights_tensor = class_weights_tensor.to(device)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_params = None\n",
    "    best_state = None\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    print(f\"Total combinations: {len(combos)}\")\n",
    "\n",
    "    for combo in combos:\n",
    "        params = dict(zip(param_grid.keys(), combo))\n",
    "        print(\"\\nTrying params:\", params)\n",
    "\n",
    "        # recreate loaders with chosen batch size\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(dim=input_dim, num_classes=num_classes).to(device)\n",
    "        if params[\"optimizer\"] == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        elif params[\"optimizer\"] == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=params[\"lr\"], momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "        criterion = FocalLoss(alpha=class_weights_tensor, gamma=params.get(\"gamma\", 2.0))\n",
    "        epochs_no_improve = 0\n",
    "        local_best = 0.0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * xb.size(0)\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            preds, trues = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    _, p = torch.max(out, dim=1)\n",
    "                    preds.extend(p.cpu().numpy()); trues.extend(yb.cpu().numpy())\n",
    "\n",
    "            val_f1 = f1_score(trues, preds, average=\"weighted\")\n",
    "            print(f\"  epoch {epoch} train_loss: {train_loss:.4f} val_f1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > local_best:\n",
    "                local_best = val_f1\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"  early stopping\")\n",
    "                    break\n",
    "\n",
    "        # end epochs for this combo\n",
    "        if local_best > best_f1:\n",
    "            best_f1 = local_best\n",
    "            best_params = params\n",
    "            best_state = model.state_dict().copy()\n",
    "            torch.save(best_state, save_path)\n",
    "            print(\"  -> New global best. Saved model.\")\n",
    "\n",
    "    print(f\"\\nGlobal best val F1: {best_f1:.4f} params: {best_params}\")\n",
    "    return best_params, best_f1, best_state\n",
    "\n",
    "# Build datasets (reuse existing X_train_res, y_train_res, X_val, y_val)\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_res.astype(np.float32)), torch.from_numpy(y_train_res.astype(np.int64)))\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val.astype(np.float32)), torch.from_numpy(y_val.astype(np.int64)))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test.astype(np.float32)), torch.from_numpy(y_test.astype(np.int64)))\n",
    "\n",
    "input_dim = X_train_res.shape[1]\n",
    "num_classes = int(np.unique(y).size)\n",
    "\n",
    "# Recommended small grid to start — tune and then expand if needed\n",
    "param_grid = {\n",
    "    \"lr\": [1e-3, 5e-4],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"]\n",
    "}\n",
    "\n",
    "best_params, best_f1, best_state = train_with_hyperparameter_tuning(\n",
    "    model_class=HybridNODEAttentionModel,\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    class_weights_tensor=class_weights_tensor,\n",
    "    param_grid=param_grid,\n",
    "    save_path=OUT_MODEL,\n",
    "    epochs=8,\n",
    "    patience=2,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Best params:\", best_params, \"Best val F1:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d73c6",
   "metadata": {},
   "source": [
    "### Final evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53412516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model state from training.\n",
      "Test class counts: [950 950]\n",
      "Accuracy: 0.8657894736842106\n",
      "F1 (weighted): 0.8657831904042046\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          SR       0.86      0.87      0.87       950\n",
      "          AF       0.87      0.86      0.86       950\n",
      "\n",
      "    accuracy                           0.87      1900\n",
      "   macro avg       0.87      0.87      0.87      1900\n",
      "weighted avg       0.87      0.87      0.87      1900\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[829 121]\n",
      " [134 816]]\n"
     ]
    }
   ],
   "source": [
    "# --- Recreate model and load best weights ---\n",
    "model = HybridNODEAttentionModel(dim=input_dim, num_classes=num_classes).to(device)\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(\"Loaded best model state from training.\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(OUT_MODEL, map_location=device))\n",
    "    print(\"Loaded model state from file.\")\n",
    "\n",
    "# --- Prepare test data ---\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Evaluate ---\n",
    "model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        _, p = torch.max(out, dim=1)\n",
    "        test_preds.extend(p.cpu().numpy())\n",
    "        test_trues.extend(yb.numpy())\n",
    "\n",
    "# --- Print metrics ---\n",
    "print(\"Test class counts:\", np.bincount(test_trues))\n",
    "print(\"Accuracy:\", accuracy_score(test_trues, test_preds))\n",
    "print(\"F1 (weighted):\", f1_score(test_trues, test_preds, average=\"weighted\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(test_trues, test_preds, target_names=[\"SR\", \"AF\"]))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(test_trues, test_preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
