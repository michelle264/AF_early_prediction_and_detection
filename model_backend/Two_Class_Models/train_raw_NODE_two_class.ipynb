{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2e653d",
   "metadata": {},
   "source": [
    "### import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b66d9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_PATH = os.path.join(\"..\", \"Dataset\", \"raw_RRI_segments.csv\")\n",
    "OUT_MODEL = os.path.join(\"..\", \"Two_Class_Models/saved_models\", \"NODE_raw_two_class_best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b037d35",
   "metadata": {},
   "source": [
    "### Load PSR dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b232ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded raw samples: (9497, 50) class counts: [4750 4747]\n",
      "Unique labels: [0 1]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# Keep only classes 0 (SR) and 2 (AF)\n",
    "mask_binary = df[\"label\"].isin([0, 2])\n",
    "df = df[mask_binary].copy()\n",
    "\n",
    "# Remap labels: 0 → 0 (SR), 2 → 1 (AF)\n",
    "df[\"label\"] = df[\"label\"].map({0: 0, 2: 1})\n",
    "\n",
    "# Pick raw RRI columns (prefix 'r_')\n",
    "feature_cols = [c for c in df.columns if c.startswith(\"r_\")]\n",
    "meta_cols = [\"patient_id\", \"record_id\", \"label\", \"label_str\", \"orig_len\"]\n",
    "\n",
    "assert len(feature_cols) > 0, \"No r_ columns found in CSV\"\n",
    "\n",
    "# Extract feature matrix and labels\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "y = df[\"label\"].values.astype(int)\n",
    "\n",
    "# Drop rows with NaN or Inf\n",
    "mask_good = np.isfinite(X).all(axis=1)\n",
    "X = X[mask_good]\n",
    "y = y[mask_good]\n",
    "\n",
    "print(\"Loaded raw samples:\", X.shape, \"class counts:\", np.bincount(y))\n",
    "print(\"Unique labels:\", np.unique(y))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be49de3",
   "metadata": {},
   "source": [
    "### Train / val / test split, scaling and SMOTE (apply SMOTE only to training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7395a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits -> train: (6077, 50) val: (1520, 50) test: (1900, 50)\n",
      "After SMOTE train distribution: [3040 3040]\n"
     ]
    }
   ],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"Splits -> train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
    "\n",
    "# Standardize using training stats\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# SMOTE on training only\n",
    "smote = SMOTE(random_state=RANDOM_SEED)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE train distribution:\", np.bincount(y_train_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d412a",
   "metadata": {},
   "source": [
    "### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43c4e425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# compute class weights from resampled training set (used as alpha for focal loss)\n",
    "cw = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_res), y=y_train_res)\n",
    "class_weights_tensor = torch.tensor(cw, dtype=torch.float32)\n",
    "print(\"Class weights:\", cw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94d44a",
   "metadata": {},
   "source": [
    "### Compute focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95e2ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf97a5a",
   "metadata": {},
   "source": [
    "### Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2adfa881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class NODEModel(nn.Module):\n",
    "    def __init__(self, dim, num_classes):\n",
    "        super(NODEModel, self).__init__()\n",
    "        self.odefunc = ODEFunc(dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, features) — treat as initial state\n",
    "        t = torch.tensor([0.0, 1.0], dtype=x.dtype, device=x.device)\n",
    "        out = odeint(self.odefunc, x, t)[-1]\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c8d37",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bbb20fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a48ed8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 8\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.0999 val_f1: 0.8460\n",
      "  epoch 2 train_loss: 0.0909 val_f1: 0.8644\n",
      "  epoch 3 train_loss: 0.0862 val_f1: 0.8558\n",
      "  epoch 4 train_loss: 0.0823 val_f1: 0.8650\n",
      "  epoch 5 train_loss: 0.0798 val_f1: 0.8651\n",
      "  epoch 6 train_loss: 0.0765 val_f1: 0.8644\n",
      "  epoch 7 train_loss: 0.0739 val_f1: 0.8664\n",
      "  epoch 8 train_loss: 0.0711 val_f1: 0.8631\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 16, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1040 val_f1: 0.8361\n",
      "  epoch 2 train_loss: 0.0982 val_f1: 0.8361\n",
      "  epoch 3 train_loss: 0.0964 val_f1: 0.8413\n",
      "  epoch 4 train_loss: 0.0952 val_f1: 0.8440\n",
      "  epoch 5 train_loss: 0.0943 val_f1: 0.8492\n",
      "  epoch 6 train_loss: 0.0934 val_f1: 0.8498\n",
      "  epoch 7 train_loss: 0.0926 val_f1: 0.8512\n",
      "  epoch 8 train_loss: 0.0919 val_f1: 0.8473\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.0997 val_f1: 0.8459\n",
      "  epoch 2 train_loss: 0.0905 val_f1: 0.8664\n",
      "  epoch 3 train_loss: 0.0866 val_f1: 0.8583\n",
      "  epoch 4 train_loss: 0.0824 val_f1: 0.8691\n",
      "  epoch 5 train_loss: 0.0798 val_f1: 0.8724\n",
      "  epoch 6 train_loss: 0.0767 val_f1: 0.8645\n",
      "  epoch 7 train_loss: 0.0737 val_f1: 0.8651\n",
      "  early stopping\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1125 val_f1: 0.8355\n",
      "  epoch 2 train_loss: 0.0997 val_f1: 0.8321\n",
      "  epoch 3 train_loss: 0.0981 val_f1: 0.8320\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.0987 val_f1: 0.8503\n",
      "  epoch 2 train_loss: 0.0912 val_f1: 0.8637\n",
      "  epoch 3 train_loss: 0.0865 val_f1: 0.8558\n",
      "  epoch 4 train_loss: 0.0829 val_f1: 0.8562\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1126 val_f1: 0.8283\n",
      "  epoch 2 train_loss: 0.1004 val_f1: 0.8368\n",
      "  epoch 3 train_loss: 0.0986 val_f1: 0.8334\n",
      "  epoch 4 train_loss: 0.0975 val_f1: 0.8327\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.0992 val_f1: 0.8401\n",
      "  epoch 2 train_loss: 0.0934 val_f1: 0.8462\n",
      "  epoch 3 train_loss: 0.0900 val_f1: 0.8584\n",
      "  epoch 4 train_loss: 0.0868 val_f1: 0.8624\n",
      "  epoch 5 train_loss: 0.0839 val_f1: 0.8684\n",
      "  epoch 6 train_loss: 0.0822 val_f1: 0.8595\n",
      "  epoch 7 train_loss: 0.0799 val_f1: 0.8677\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1185 val_f1: 0.8177\n",
      "  epoch 2 train_loss: 0.1044 val_f1: 0.8303\n",
      "  epoch 3 train_loss: 0.1023 val_f1: 0.8348\n",
      "  epoch 4 train_loss: 0.1011 val_f1: 0.8368\n",
      "  epoch 5 train_loss: 0.1002 val_f1: 0.8341\n",
      "  epoch 6 train_loss: 0.0996 val_f1: 0.8341\n",
      "  early stopping\n",
      "\n",
      "Global best val F1: 0.8724 params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "Best params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'} Best val F1: 0.87236753717131\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_with_hyperparameter_tuning(model_class, input_dim, num_classes,\n",
    "                                     train_dataset, val_dataset,\n",
    "                                     class_weights_tensor,\n",
    "                                     param_grid,\n",
    "                                     save_path=OUT_MODEL,\n",
    "                                     epochs=8,\n",
    "                                     patience=2,\n",
    "                                     device=None):\n",
    "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    class_weights_tensor = class_weights_tensor.to(device)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_params = None\n",
    "    best_state = None\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    print(f\"Total combinations: {len(combos)}\")\n",
    "\n",
    "    for combo in combos:\n",
    "        params = dict(zip(param_grid.keys(), combo))\n",
    "        print(\"\\nTrying params:\", params)\n",
    "\n",
    "        # recreate loaders with chosen batch size\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(dim=input_dim, num_classes=num_classes).to(device)\n",
    "        if params[\"optimizer\"] == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        elif params[\"optimizer\"] == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=params[\"lr\"], momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "        criterion = FocalLoss(alpha=class_weights_tensor, gamma=params.get(\"gamma\", 2.0))\n",
    "        epochs_no_improve = 0\n",
    "        local_best = 0.0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * xb.size(0)\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            preds, trues = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    _, p = torch.max(out, dim=1)\n",
    "                    preds.extend(p.cpu().numpy()); trues.extend(yb.cpu().numpy())\n",
    "\n",
    "            val_f1 = f1_score(trues, preds, average=\"weighted\")\n",
    "            print(f\"  epoch {epoch} train_loss: {train_loss:.4f} val_f1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > local_best:\n",
    "                local_best = val_f1\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"  early stopping\")\n",
    "                    break\n",
    "\n",
    "        # end epochs for this combo\n",
    "        if local_best > best_f1:\n",
    "            best_f1 = local_best\n",
    "            best_params = params\n",
    "            best_state = model.state_dict().copy()\n",
    "            torch.save(best_state, save_path)\n",
    "            print(\"  -> New global best. Saved model.\")\n",
    "\n",
    "    print(f\"\\nGlobal best val F1: {best_f1:.4f} params: {best_params}\")\n",
    "    return best_params, best_f1, best_state\n",
    "\n",
    "# Build datasets (reuse existing X_train_res, y_train_res, X_val, y_val)\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_res.astype(np.float32)), torch.from_numpy(y_train_res.astype(np.int64)))\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val.astype(np.float32)), torch.from_numpy(y_val.astype(np.int64)))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test.astype(np.float32)), torch.from_numpy(y_test.astype(np.int64)))\n",
    "\n",
    "input_dim = X_train_res.shape[1]\n",
    "num_classes = int(np.unique(y).size)\n",
    "\n",
    "# Recommended small grid to start — tune and then expand if needed\n",
    "param_grid = {\n",
    "    \"lr\": [1e-3, 5e-4],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"]\n",
    "    }\n",
    "\n",
    "best_params, best_f1, best_state = train_with_hyperparameter_tuning(\n",
    "    model_class=NODEModel,\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    class_weights_tensor=class_weights_tensor,\n",
    "    param_grid=param_grid,\n",
    "    save_path=OUT_MODEL,\n",
    "    epochs=8,\n",
    "    patience=2\n",
    ")\n",
    "\n",
    "print(\"Best params:\", best_params, \"Best val F1:\", best_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d73c6",
   "metadata": {},
   "source": [
    "### Final evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53412516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test class counts: [950 950]\n",
      "Accuracy: 0.8689473684210526\n",
      "F1 (weighted): 0.8689412329829291\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          SR       0.86      0.88      0.87       950\n",
      "          AF       0.87      0.86      0.87       950\n",
      "\n",
      "    accuracy                           0.87      1900\n",
      "   macro avg       0.87      0.87      0.87      1900\n",
      "weighted avg       0.87      0.87      0.87      1900\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[832 118]\n",
      " [131 819]]\n"
     ]
    }
   ],
   "source": [
    "# Load best model for final evaluation\n",
    "if best_state is not None:\n",
    "    best_model = NODEModel(dim=input_dim, num_classes=num_classes)\n",
    "    best_model.load_state_dict(best_state)\n",
    "    best_model = best_model.to(device)\n",
    "else:\n",
    "    # fallback: instantiate a fresh model\n",
    "    best_model = NODEModel(dim=input_dim, num_classes=num_classes).to(device)\n",
    "\n",
    "# ensure test_loader exists (create if needed)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Final evaluation\n",
    "best_model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = best_model(xb)\n",
    "        _, p = torch.max(out, dim=1)\n",
    "        test_preds.extend(p.cpu().numpy())\n",
    "        test_trues.extend(yb.numpy())\n",
    "\n",
    "print(\"Test class counts:\", np.bincount(test_trues))\n",
    "print(\"Accuracy:\", accuracy_score(test_trues, test_preds))\n",
    "print(\"F1 (weighted):\", f1_score(test_trues, test_preds, average=\"weighted\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(test_trues, test_preds, target_names=[\"SR\", \"AF\"]))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(test_trues, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b114a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
