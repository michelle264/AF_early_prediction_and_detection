{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2e653d",
   "metadata": {},
   "source": [
    "### import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b66d9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_PATH = os.path.join(\"..\", \"Dataset\", \"raw_RRI_segments.csv\")\n",
    "OUT_MODEL = os.path.join(\"..\", \"Two_Class_Models/saved_models\", \"NODE_PSR_two_class_best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b037d35",
   "metadata": {},
   "source": [
    "### Load PSR dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b232ed1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV loaded. Shape: (14205, 55)\n",
      "Filtered classes 0 & 2. Shape: (9497, 55)\n",
      "Class distribution:\n",
      " label\n",
      "0    4750\n",
      "1    4747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"CSV loaded. Shape:\", df.shape)\n",
    "\n",
    "# Keep only classes 0 (SR) and 2 (AF)\n",
    "mask_binary = df[\"label\"].isin([0, 2])\n",
    "df = df[mask_binary].copy()\n",
    "\n",
    "# Remap 0->0, 2->1 for binary classification\n",
    "df[\"label\"] = df[\"label\"].map({0: 0, 2: 1})\n",
    "\n",
    "print(\"Filtered classes 0 & 2. Shape:\", df.shape)\n",
    "print(\"Class distribution:\\n\", df[\"label\"].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e55c26a",
   "metadata": {},
   "source": [
    "### PSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d74dcf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSR features added (m=3, tau=2). DataFrame shape: (9497, 193)\n"
     ]
    }
   ],
   "source": [
    "def phase_space_reconstruct(x, m=3, tau=1):\n",
    "    \"\"\"\n",
    "    x: 1D array of RRI\n",
    "    m: embedding dimension\n",
    "    tau: time delay\n",
    "    Returns flattened PSR embedding: x(t), x(t+tau), ..., x(t+(m-1)*tau)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    N = len(x)\n",
    "    if N < (m-1)*tau + 1:\n",
    "        # pad with zeros if too short\n",
    "        x = np.pad(x, (0, (m-1)*tau + 1 - N), 'constant')\n",
    "        N = len(x)\n",
    "    psr_vectors = [x[i:N-(m-1)*tau + i] for i in range(m)]\n",
    "    psr_flat = np.column_stack(psr_vectors).flatten()\n",
    "    return psr_flat\n",
    "\n",
    "# Pick RRI columns\n",
    "rri_cols = [f\"r_{i}\" for i in range(50)]\n",
    "\n",
    "# Compute PSR for all rows with m=3, tau=2\n",
    "psr_features = df[rri_cols].apply(lambda row: phase_space_reconstruct(row.values, m=3, tau=2), axis=1)\n",
    "psr_features = np.stack(psr_features.values)  # shape: (num_samples, m*(N-(m-1)*tau))\n",
    "\n",
    "# Add PSR columns to DataFrame\n",
    "num_psr_cols = psr_features.shape[1]\n",
    "psr_col_names = [f\"psr_{i}\" for i in range(num_psr_cols)]\n",
    "df_psr = pd.DataFrame(psr_features, columns=psr_col_names, index=df.index)\n",
    "df = pd.concat([df, df_psr], axis=1)\n",
    "\n",
    "print(\"PSR features added (m=3, tau=2). DataFrame shape:\", df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c85f9c",
   "metadata": {},
   "source": [
    "### Extract features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd685ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded samples: (9497, 138) class counts: [4750 4747]\n"
     ]
    }
   ],
   "source": [
    "feature_cols = psr_col_names\n",
    "meta_cols = [\"patient_id\", \"record_id\", \"label\", \"label_str\", \"orig_len\"]\n",
    "\n",
    "# Feature matrix X and labels y\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "y = df[\"label\"].values.astype(int)\n",
    "\n",
    "# Drop rows with NaN or inf\n",
    "mask_good = np.isfinite(X).all(axis=1)\n",
    "X = X[mask_good]\n",
    "y = y[mask_good]\n",
    "\n",
    "print(\"Loaded samples:\", X.shape, \"class counts:\", np.bincount(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be49de3",
   "metadata": {},
   "source": [
    "### Train / val / test split, scaling and SMOTE (apply SMOTE only to training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7395a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits -> train: (6077, 138) val: (1520, 138) test: (1900, 138)\n",
      "After SMOTE train distribution: [3040 3040]\n"
     ]
    }
   ],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"Splits -> train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
    "\n",
    "# Standardize using training stats\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# SMOTE on training only\n",
    "smote = SMOTE(random_state=RANDOM_SEED)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE train distribution:\", np.bincount(y_train_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d412a",
   "metadata": {},
   "source": [
    "### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43c4e425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# compute class weights from resampled training set (used as alpha for focal loss)\n",
    "cw = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_res), y=y_train_res)\n",
    "class_weights_tensor = torch.tensor(cw, dtype=torch.float32)\n",
    "print(\"Class weights:\", cw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94d44a",
   "metadata": {},
   "source": [
    "### Compute focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e2ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf97a5a",
   "metadata": {},
   "source": [
    "### Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2adfa881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class NODEModel(nn.Module):\n",
    "    def __init__(self, dim, num_classes):\n",
    "        super(NODEModel, self).__init__()\n",
    "        self.odefunc = ODEFunc(dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, features) â€” treat as initial state\n",
    "        t = torch.tensor([0.0, 1.0], dtype=x.dtype, device=x.device)\n",
    "        out = odeint(self.odefunc, x, t)[-1]\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c8d37",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf92c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a48ed8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 8\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.1001 val_f1: 0.8477\n",
      "  epoch 2 train_loss: 0.0903 val_f1: 0.8465\n",
      "  epoch 3 train_loss: 0.0851 val_f1: 0.7934\n",
      "  early stopping\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 16, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1052 val_f1: 0.8374\n",
      "  epoch 2 train_loss: 0.0963 val_f1: 0.8393\n",
      "  epoch 3 train_loss: 0.0945 val_f1: 0.8418\n",
      "  epoch 4 train_loss: 0.0931 val_f1: 0.8431\n",
      "  epoch 5 train_loss: 0.0920 val_f1: 0.8512\n",
      "  epoch 6 train_loss: 0.0910 val_f1: 0.8498\n",
      "  epoch 7 train_loss: 0.0899 val_f1: 0.8552\n",
      "  epoch 8 train_loss: 0.0889 val_f1: 0.8578\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.0989 val_f1: 0.8550\n",
      "  epoch 2 train_loss: 0.0898 val_f1: 0.8519\n",
      "  epoch 3 train_loss: 0.0840 val_f1: 0.8605\n",
      "  epoch 4 train_loss: 0.0800 val_f1: 0.8651\n",
      "  epoch 5 train_loss: 0.0747 val_f1: 0.8683\n",
      "  epoch 6 train_loss: 0.0704 val_f1: 0.8730\n",
      "  epoch 7 train_loss: 0.0679 val_f1: 0.8637\n",
      "  epoch 8 train_loss: 0.0628 val_f1: 0.8783\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1117 val_f1: 0.8368\n",
      "  epoch 2 train_loss: 0.0997 val_f1: 0.8347\n",
      "  epoch 3 train_loss: 0.0978 val_f1: 0.8340\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.0990 val_f1: 0.8447\n",
      "  epoch 2 train_loss: 0.0912 val_f1: 0.8664\n",
      "  epoch 3 train_loss: 0.0864 val_f1: 0.8585\n",
      "  epoch 4 train_loss: 0.0829 val_f1: 0.8644\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1106 val_f1: 0.8307\n",
      "  epoch 2 train_loss: 0.1000 val_f1: 0.8353\n",
      "  epoch 3 train_loss: 0.0977 val_f1: 0.8374\n",
      "  epoch 4 train_loss: 0.0961 val_f1: 0.8346\n",
      "  epoch 5 train_loss: 0.0949 val_f1: 0.8394\n",
      "  epoch 6 train_loss: 0.0940 val_f1: 0.8413\n",
      "  epoch 7 train_loss: 0.0931 val_f1: 0.8426\n",
      "  epoch 8 train_loss: 0.0923 val_f1: 0.8426\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.0980 val_f1: 0.8399\n",
      "  epoch 2 train_loss: 0.0899 val_f1: 0.8611\n",
      "  epoch 3 train_loss: 0.0850 val_f1: 0.8611\n",
      "  epoch 4 train_loss: 0.0821 val_f1: 0.8631\n",
      "  epoch 5 train_loss: 0.0791 val_f1: 0.8704\n",
      "  epoch 6 train_loss: 0.0757 val_f1: 0.8691\n",
      "  epoch 7 train_loss: 0.0730 val_f1: 0.8697\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.1205 val_f1: 0.8151\n",
      "  epoch 2 train_loss: 0.1011 val_f1: 0.8255\n",
      "  epoch 3 train_loss: 0.0993 val_f1: 0.8294\n",
      "  epoch 4 train_loss: 0.0979 val_f1: 0.8328\n",
      "  epoch 5 train_loss: 0.0970 val_f1: 0.8327\n",
      "  epoch 6 train_loss: 0.0961 val_f1: 0.8307\n",
      "  early stopping\n",
      "\n",
      "Global best val F1: 0.8783 params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "Best params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'} Best val F1: 0.8782662376258626\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_with_hyperparameter_tuning(model_class, input_dim, num_classes,\n",
    "                                     train_dataset, val_dataset,\n",
    "                                     class_weights_tensor,\n",
    "                                     param_grid,\n",
    "                                     save_path=OUT_MODEL,\n",
    "                                     epochs=8,\n",
    "                                     patience=2,\n",
    "                                     device=device):\n",
    "    class_weights_tensor = class_weights_tensor.to(device)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_params = None\n",
    "    best_state = None\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    print(f\"Total combinations: {len(combos)}\")\n",
    "\n",
    "    for combo in combos:\n",
    "        params = dict(zip(param_grid.keys(), combo))\n",
    "        print(\"\\nTrying params:\", params)\n",
    "\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(dim=input_dim, num_classes=num_classes).to(device)\n",
    "        if params[\"optimizer\"] == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        elif params[\"optimizer\"] == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=params[\"lr\"], momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "        criterion = FocalLoss(alpha=class_weights_tensor, gamma=params.get(\"gamma\", 2.0))\n",
    "        epochs_no_improve = 0\n",
    "        local_best = 0.0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * xb.size(0)\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            preds, trues = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    _, p = torch.max(out, dim=1)\n",
    "                    preds.extend(p.cpu().numpy()); trues.extend(yb.cpu().numpy())\n",
    "\n",
    "            val_f1 = f1_score(trues, preds, average=\"weighted\")\n",
    "            print(f\"  epoch {epoch} train_loss: {train_loss:.4f} val_f1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > local_best:\n",
    "                local_best = val_f1\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"  early stopping\")\n",
    "                    break\n",
    "\n",
    "        # update global best\n",
    "        if local_best > best_f1:\n",
    "            best_f1 = local_best\n",
    "            best_params = params\n",
    "            best_state = model.state_dict().copy()\n",
    "            torch.save(best_state, save_path)\n",
    "            print(\"  -> New global best. Saved model.\")\n",
    "\n",
    "    print(f\"\\nGlobal best val F1: {best_f1:.4f} params: {best_params}\")\n",
    "    return best_params, best_f1, best_state\n",
    "\n",
    "# Build datasets (reuse existing X_train_res, y_train_res, X_val, y_val)\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_res.astype(np.float32)), torch.from_numpy(y_train_res.astype(np.int64)))\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val.astype(np.float32)), torch.from_numpy(y_val.astype(np.int64)))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test.astype(np.float32)), torch.from_numpy(y_test.astype(np.int64)))\n",
    "\n",
    "input_dim = X_train_res.shape[1]\n",
    "num_classes = int(np.unique(y).size)\n",
    "\n",
    "param_grid = {\n",
    "    \"lr\": [1e-3, 5e-4],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"]\n",
    "}\n",
    "\n",
    "best_params, best_f1, best_state = train_with_hyperparameter_tuning(\n",
    "    model_class=NODEModel,\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    class_weights_tensor=class_weights_tensor,\n",
    "    param_grid=param_grid,\n",
    "    save_path=OUT_MODEL,\n",
    "    epochs=8,\n",
    "    patience=2,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Best params:\", best_params, \"Best val F1:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d73c6",
   "metadata": {},
   "source": [
    "### Final evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53412516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test class counts: [950 950]\n",
      "Accuracy: 0.8757894736842106\n",
      "F1 (weighted): 0.8757584993488682\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          SR       0.89      0.86      0.87       950\n",
      "          AF       0.86      0.89      0.88       950\n",
      "\n",
      "    accuracy                           0.88      1900\n",
      "   macro avg       0.88      0.88      0.88      1900\n",
      "weighted avg       0.88      0.88      0.88      1900\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[817 133]\n",
      " [103 847]]\n"
     ]
    }
   ],
   "source": [
    "# Load best model for final evaluation\n",
    "if best_state is not None:\n",
    "    best_model = NODEModel(dim=input_dim, num_classes=num_classes)\n",
    "    best_model.load_state_dict(best_state)\n",
    "    best_model = best_model.to(device)\n",
    "else:\n",
    "    # fallback: instantiate a fresh model\n",
    "    best_model = NODEModel(dim=input_dim, num_classes=num_classes).to(device)\n",
    "\n",
    "# ensure test_loader exists (create if needed)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Final evaluation\n",
    "best_model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = best_model(xb)\n",
    "        _, p = torch.max(out, dim=1)\n",
    "        test_preds.extend(p.cpu().numpy())\n",
    "        test_trues.extend(yb.numpy())\n",
    "\n",
    "print(\"Test class counts:\", np.bincount(test_trues))\n",
    "print(\"Accuracy:\", accuracy_score(test_trues, test_preds))\n",
    "print(\"F1 (weighted):\", f1_score(test_trues, test_preds, average=\"weighted\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(test_trues, test_preds, target_names=[\"SR\",\"AF\"]))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(test_trues, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7c1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
