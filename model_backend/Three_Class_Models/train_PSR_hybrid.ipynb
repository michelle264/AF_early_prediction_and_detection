{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2e653d",
   "metadata": {},
   "source": [
    "### import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b66d9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_PATH = os.path.join(\"..\", \"Dataset\", \"raw_RRI_segments.csv\")\n",
    "OUT_MODEL = os.path.join(\"..\", \"Three_Class_Models\", \"saved_models\", \"PSR_hybrid_best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b037d35",
   "metadata": {},
   "source": [
    "### Load PSR dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b232ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "# pick RRI feature columns (prefix 'r_')\n",
    "feature_cols = [c for c in df.columns if c.startswith(\"r_\")]\n",
    "meta_cols = [\"patient_id\", \"record_id\", \"label\", \"label_str\", \"orig_len\"]\n",
    "\n",
    "assert len(feature_cols) > 0, \"No r_ columns found in CSV\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24e6e704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSR features added (m=3, tau=2). DataFrame shape: (14205, 193)\n"
     ]
    }
   ],
   "source": [
    "def phase_space_reconstruct(x, m=3, tau=1):\n",
    "    \"\"\"\n",
    "    x: 1D array of RRI\n",
    "    m: embedding dimension\n",
    "    tau: time delay\n",
    "    Returns flattened PSR embedding: x(t), x(t+tau), ..., x(t+(m-1)*tau)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    N = len(x)\n",
    "    if N < (m-1)*tau + 1:\n",
    "        # pad with zeros if too short\n",
    "        x = np.pad(x, (0, (m-1)*tau + 1 - N), 'constant')\n",
    "        N = len(x)\n",
    "    psr_vectors = [x[i:N-(m-1)*tau + i] for i in range(m)]\n",
    "    psr_flat = np.column_stack(psr_vectors).flatten()\n",
    "    return psr_flat\n",
    "\n",
    "# Pick RRI columns\n",
    "rri_cols = [f\"r_{i}\" for i in range(50)]\n",
    "\n",
    "# Compute PSR for all rows with m=3, tau=2\n",
    "psr_features = df[rri_cols].apply(lambda row: phase_space_reconstruct(row.values, m=3, tau=2), axis=1)\n",
    "psr_features = np.stack(psr_features.values)  # shape: (num_samples, m*(N-(m-1)*tau))\n",
    "\n",
    "# Add PSR columns to DataFrame\n",
    "num_psr_cols = psr_features.shape[1]\n",
    "psr_col_names = [f\"psr_{i}\" for i in range(num_psr_cols)]\n",
    "df_psr = pd.DataFrame(psr_features, columns=psr_col_names, index=df.index)\n",
    "df = pd.concat([df, df_psr], axis=1)\n",
    "\n",
    "print(\"PSR features added (m=3, tau=2). DataFrame shape:\", df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9116ba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Loaded samples: (14204, 138) class counts: [4750 4707 4747]\n"
     ]
    }
   ],
   "source": [
    "feature_cols = psr_col_names\n",
    "meta_cols = [\"patient_id\", \"record_id\", \"label\", \"label_str\", \"orig_len\"]\n",
    "\n",
    "# Feature matrix X\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "\n",
    "# Keep label as float first (to handle NaN/inf safely)\n",
    "y_raw = df[\"label\"].values\n",
    "\n",
    "# Step 1: Drop rows with invalid X (NaN or inf)\n",
    "mask_good = np.isfinite(X).all(axis=1)\n",
    "\n",
    "# Step 2: Drop invalid labels (NaN, inf, negative)\n",
    "mask_good &= np.isfinite(y_raw) & (y_raw >= 0)\n",
    "\n",
    "# Apply combined mask\n",
    "X = X[mask_good]\n",
    "y = y_raw[mask_good].astype(int)\n",
    "\n",
    "# Step 3: Inspect\n",
    "print(np.unique(y)[:10])   # print first few unique labels\n",
    "print(y[:20])\n",
    "\n",
    "# Step 4: Safe bincount\n",
    "print(\"Loaded samples:\", X.shape, \"class counts:\", np.bincount(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be49de3",
   "metadata": {},
   "source": [
    "### Train / val / test split, scaling and SMOTE (apply SMOTE only to training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7395a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits -> train: (9090, 138) val: (2273, 138) test: (2841, 138)\n",
      "After SMOTE train distribution: [3040 3040 3040]\n"
     ]
    }
   ],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"Splits -> train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
    "\n",
    "# Standardize using training stats\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# SMOTE on training only\n",
    "smote = SMOTE(random_state=RANDOM_SEED)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE train distribution:\", np.bincount(y_train_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d412a",
   "metadata": {},
   "source": [
    "### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c4e425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# compute class weights from resampled training set (used as alpha for focal loss)\n",
    "cw = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_res), y=y_train_res)\n",
    "class_weights_tensor = torch.tensor(cw, dtype=torch.float32)\n",
    "print(\"Class weights:\", cw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94d44a",
   "metadata": {},
   "source": [
    "### Compute focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95e2ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf97a5a",
   "metadata": {},
   "source": [
    "### Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2adfa881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model definitions (Hybrid NODE + Attention) ---\n",
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, dim)\n",
    "        )\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(dim, dim)\n",
    "        self.key = nn.Linear(dim, dim)\n",
    "        self.value = nn.Linear(dim, dim)\n",
    "        self.scale = dim ** 0.5\n",
    "    def forward(self, x):\n",
    "        # x: [batch, dim] -> [batch, 1, dim]\n",
    "        x1 = x.unsqueeze(1)\n",
    "        Q = self.query(x1)\n",
    "        K = self.key(x1)\n",
    "        V = self.value(x1)\n",
    "        scores = torch.softmax(torch.bmm(Q, K.transpose(1,2)) / self.scale, dim=-1)\n",
    "        out = torch.bmm(scores, V)  # [batch,1,dim]\n",
    "        return out.squeeze(1)\n",
    "\n",
    "class HybridNODEAttentionModel(nn.Module):\n",
    "    def __init__(self, dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.odefunc = ODEFunc(dim)\n",
    "        self.attn = SelfAttention(dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x: [batch, dim]\n",
    "        t = torch.tensor([0.0, 1.0], dtype=x.dtype, device=x.device)\n",
    "        ode_out = odeint(self.odefunc, x, t)[-1]\n",
    "        attn_out = self.attn(ode_out)\n",
    "        return self.classifier(attn_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa97a4a",
   "metadata": {},
   "source": [
    "### Prepare DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef3e112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train_res.astype(np.float32)), torch.from_numpy(y_train_res.astype(np.int64)))\n",
    "val_ds = TensorDataset(torch.from_numpy(X_val.astype(np.float32)), torch.from_numpy(y_val.astype(np.int64)))\n",
    "test_ds = TensorDataset(torch.from_numpy(X_test.astype(np.float32)), torch.from_numpy(y_test.astype(np.int64)))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = X_train_res.shape[1]\n",
    "num_classes = int(np.unique(y).size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c8d37",
   "metadata": {},
   "source": [
    "### Train model (hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a48ed8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 8\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.3270 val_f1: 0.5306\n",
      "  epoch 2 train_loss: 0.3105 val_f1: 0.5998\n",
      "  epoch 3 train_loss: 0.2999 val_f1: 0.5191\n",
      "  epoch 4 train_loss: 0.2897 val_f1: 0.5738\n",
      "  early stopping\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 16, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.3372 val_f1: 0.5782\n",
      "  epoch 2 train_loss: 0.3190 val_f1: 0.5501\n",
      "  epoch 3 train_loss: 0.3153 val_f1: 0.5806\n",
      "  epoch 4 train_loss: 0.3127 val_f1: 0.5321\n",
      "  epoch 5 train_loss: 0.3098 val_f1: 0.5681\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.3256 val_f1: 0.5685\n",
      "  epoch 2 train_loss: 0.3062 val_f1: 0.6057\n",
      "  epoch 3 train_loss: 0.2951 val_f1: 0.5840\n",
      "  epoch 4 train_loss: 0.2878 val_f1: 0.6043\n",
      "  early stopping\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.3462 val_f1: 0.5758\n",
      "  epoch 2 train_loss: 0.3226 val_f1: 0.5769\n",
      "  epoch 3 train_loss: 0.3189 val_f1: 0.5777\n",
      "  epoch 4 train_loss: 0.3166 val_f1: 0.5874\n",
      "  epoch 5 train_loss: 0.3152 val_f1: 0.5877\n",
      "  epoch 6 train_loss: 0.3138 val_f1: 0.5790\n",
      "  epoch 7 train_loss: 0.3125 val_f1: 0.5843\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.3260 val_f1: 0.5759\n",
      "  epoch 2 train_loss: 0.3076 val_f1: 0.5801\n",
      "  epoch 3 train_loss: 0.2953 val_f1: 0.5864\n",
      "  epoch 4 train_loss: 0.2861 val_f1: 0.5965\n",
      "  epoch 5 train_loss: 0.2789 val_f1: 0.6086\n",
      "  epoch 6 train_loss: 0.2708 val_f1: 0.6023\n",
      "  epoch 7 train_loss: 0.2648 val_f1: 0.6034\n",
      "  early stopping\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.3472 val_f1: 0.5587\n",
      "  epoch 2 train_loss: 0.3213 val_f1: 0.5736\n",
      "  epoch 3 train_loss: 0.3178 val_f1: 0.5813\n",
      "  epoch 4 train_loss: 0.3156 val_f1: 0.5845\n",
      "  epoch 5 train_loss: 0.3142 val_f1: 0.5856\n",
      "  epoch 6 train_loss: 0.3129 val_f1: 0.5846\n",
      "  epoch 7 train_loss: 0.3113 val_f1: 0.5790\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.3256 val_f1: 0.5626\n",
      "  epoch 2 train_loss: 0.3115 val_f1: 0.5793\n",
      "  epoch 3 train_loss: 0.3002 val_f1: 0.5930\n",
      "  epoch 4 train_loss: 0.2905 val_f1: 0.6019\n",
      "  epoch 5 train_loss: 0.2852 val_f1: 0.5946\n",
      "  epoch 6 train_loss: 0.2756 val_f1: 0.5900\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.3665 val_f1: 0.5666\n",
      "  epoch 2 train_loss: 0.3252 val_f1: 0.5809\n",
      "  epoch 3 train_loss: 0.3204 val_f1: 0.5876\n",
      "  epoch 4 train_loss: 0.3180 val_f1: 0.5652\n",
      "  epoch 5 train_loss: 0.3166 val_f1: 0.5884\n",
      "  epoch 6 train_loss: 0.3155 val_f1: 0.5864\n",
      "  epoch 7 train_loss: 0.3145 val_f1: 0.5931\n",
      "  epoch 8 train_loss: 0.3138 val_f1: 0.5912\n",
      "\n",
      "Global best val F1: 0.6086 params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "Best params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'adam'} Best val F1: 0.6086127852829064\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_with_hyperparameter_tuning(model_class, input_dim, num_classes,\n",
    "                                     train_dataset, val_dataset,\n",
    "                                     class_weights_tensor,\n",
    "                                     param_grid,\n",
    "                                     save_path=OUT_MODEL,\n",
    "                                     epochs=8,\n",
    "                                     patience=2,\n",
    "                                     device=None):\n",
    "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "    class_weights_tensor = class_weights_tensor.to(device)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_params = None\n",
    "    best_state = None\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    print(f\"Total combinations: {len(combos)}\")\n",
    "\n",
    "    for combo in combos:\n",
    "        params = dict(zip(param_grid.keys(), combo))\n",
    "        print(\"\\nTrying params:\", params)\n",
    "\n",
    "        # recreate loaders with chosen batch size\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(dim=input_dim, num_classes=num_classes).to(device)\n",
    "        if params[\"optimizer\"] == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        elif params[\"optimizer\"] == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=params[\"lr\"], momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "        criterion = FocalLoss(alpha=class_weights_tensor, gamma=params.get(\"gamma\", 2.0))\n",
    "        epochs_no_improve = 0\n",
    "        local_best = 0.0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * xb.size(0)\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            preds, trues = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    _, p = torch.max(out, dim=1)\n",
    "                    preds.extend(p.cpu().numpy()); trues.extend(yb.cpu().numpy())\n",
    "\n",
    "            val_f1 = f1_score(trues, preds, average=\"weighted\")\n",
    "            print(f\"  epoch {epoch} train_loss: {train_loss:.4f} val_f1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > local_best:\n",
    "                local_best = val_f1\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"  early stopping\")\n",
    "                    break\n",
    "\n",
    "        # end epochs for this combo\n",
    "        if local_best > best_f1:\n",
    "            best_f1 = local_best\n",
    "            best_params = params\n",
    "            best_state = model.state_dict().copy()\n",
    "            torch.save(best_state, save_path)\n",
    "            print(\"  -> New global best. Saved model.\")\n",
    "\n",
    "    print(f\"\\nGlobal best val F1: {best_f1:.4f} params: {best_params}\")\n",
    "    return best_params, best_f1, best_state\n",
    "\n",
    "# Build datasets (reuse existing X_train_res, y_train_res, X_val, y_val)\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_res.astype(np.float32)), torch.from_numpy(y_train_res.astype(np.int64)))\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val.astype(np.float32)), torch.from_numpy(y_val.astype(np.int64)))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test.astype(np.float32)), torch.from_numpy(y_test.astype(np.int64)))\n",
    "\n",
    "input_dim = X_train_res.shape[1]\n",
    "num_classes = int(np.unique(y).size)\n",
    "\n",
    "# Recommended small grid to start â€” tune and then expand if needed\n",
    "param_grid = {\n",
    "    \"lr\": [1e-3, 5e-4],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"]\n",
    "}\n",
    "\n",
    "best_params, best_f1, best_state = train_with_hyperparameter_tuning(\n",
    "    model_class=HybridNODEAttentionModel,\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    class_weights_tensor=class_weights_tensor,\n",
    "    param_grid=param_grid,\n",
    "    save_path=OUT_MODEL,\n",
    "    epochs=8,\n",
    "    patience=2,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Best params:\", best_params, \"Best val F1:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d73c6",
   "metadata": {},
   "source": [
    "### Final evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53412516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model state from training.\n",
      "Test class counts: [950 941 950]\n",
      "Accuracy: 0.6047166490672299\n",
      "F1 (weighted): 0.5999347794292386\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          SR       0.51      0.49      0.50       950\n",
      "      pre-AF       0.48      0.46      0.47       941\n",
      "          AF       0.80      0.87      0.83       950\n",
      "\n",
      "    accuracy                           0.60      2841\n",
      "   macro avg       0.60      0.60      0.60      2841\n",
      "weighted avg       0.60      0.60      0.60      2841\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[462 377 111]\n",
      " [412 434  95]\n",
      " [ 40  88 822]]\n"
     ]
    }
   ],
   "source": [
    "# --- Recreate model and load best weights ---\n",
    "model = HybridNODEAttentionModel(dim=input_dim, num_classes=num_classes).to(device)\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(\"Loaded best model state from training.\")\n",
    "else:\n",
    "    model.load_state_dict(torch.load(OUT_MODEL, map_location=device))\n",
    "    print(\"Loaded model state from file.\")\n",
    "\n",
    "# --- Prepare test data ---\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- Evaluate ---\n",
    "model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        _, p = torch.max(out, dim=1)\n",
    "        test_preds.extend(p.cpu().numpy())\n",
    "        test_trues.extend(yb.numpy())\n",
    "\n",
    "# --- Print metrics ---\n",
    "print(\"Test class counts:\", np.bincount(test_trues))\n",
    "print(\"Accuracy:\", accuracy_score(test_trues, test_preds))\n",
    "print(\"F1 (weighted):\", f1_score(test_trues, test_preds, average=\"weighted\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(test_trues, test_preds, target_names=[\"SR\",\"pre-AF\",\"AF\"]))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(test_trues, test_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc5c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
