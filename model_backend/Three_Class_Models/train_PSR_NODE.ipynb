{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d2e653d",
   "metadata": {},
   "source": [
    "### import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66d9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "DATA_PATH = os.path.join(\"..\", \"Dataset\", \"raw_RRI_segments.csv\")\n",
    "OUT_MODEL = os.path.join(\"..\", \"Three_Class_Models/saved_models\", \"NODE_PSR_best.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b037d35",
   "metadata": {},
   "source": [
    "### Load PSR dataset and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b232ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "# pick RRI feature columns (prefix 'r_')\n",
    "feature_cols = [c for c in df.columns if c.startswith(\"r_\")]\n",
    "meta_cols = [\"patient_id\", \"record_id\", \"label\", \"label_str\", \"orig_len\"]\n",
    "\n",
    "assert len(feature_cols) > 0, \"No r_ columns found in CSV\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfac9b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSR features added (m=3, tau=2). DataFrame shape: (14205, 193)\n"
     ]
    }
   ],
   "source": [
    "def phase_space_reconstruct(x, m=3, tau=1):\n",
    "    \"\"\"\n",
    "    x: 1D array of RRI\n",
    "    m: embedding dimension\n",
    "    tau: time delay\n",
    "    Returns flattened PSR embedding: x(t), x(t+tau), ..., x(t+(m-1)*tau)\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    N = len(x)\n",
    "    if N < (m-1)*tau + 1:\n",
    "        # pad with zeros if too short\n",
    "        x = np.pad(x, (0, (m-1)*tau + 1 - N), 'constant')\n",
    "        N = len(x)\n",
    "    psr_vectors = [x[i:N-(m-1)*tau + i] for i in range(m)]\n",
    "    psr_flat = np.column_stack(psr_vectors).flatten()\n",
    "    return psr_flat\n",
    "\n",
    "# Pick RRI columns\n",
    "rri_cols = [f\"r_{i}\" for i in range(50)]\n",
    "\n",
    "# Compute PSR for all rows with m=3, tau=2\n",
    "psr_features = df[rri_cols].apply(lambda row: phase_space_reconstruct(row.values, m=3, tau=2), axis=1)\n",
    "psr_features = np.stack(psr_features.values)  # shape: (num_samples, m*(N-(m-1)*tau))\n",
    "\n",
    "# Add PSR columns to DataFrame\n",
    "num_psr_cols = psr_features.shape[1]\n",
    "psr_col_names = [f\"psr_{i}\" for i in range(num_psr_cols)]\n",
    "df_psr = pd.DataFrame(psr_features, columns=psr_col_names, index=df.index)\n",
    "df = pd.concat([df, df_psr], axis=1)\n",
    "\n",
    "print(\"PSR features added (m=3, tau=2). DataFrame shape:\", df.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "117055f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Loaded samples: (14204, 138) class counts: [4750 4707 4747]\n"
     ]
    }
   ],
   "source": [
    "feature_cols = psr_col_names\n",
    "meta_cols = [\"patient_id\", \"record_id\", \"label\", \"label_str\", \"orig_len\"]\n",
    "\n",
    "# Feature matrix X\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "\n",
    "# Keep label as float first (to handle NaN/inf safely)\n",
    "y_raw = df[\"label\"].values\n",
    "\n",
    "# Step 1: Drop rows with invalid X (NaN or inf)\n",
    "mask_good = np.isfinite(X).all(axis=1)\n",
    "\n",
    "# Step 2: Drop invalid labels (NaN, inf, negative)\n",
    "mask_good &= np.isfinite(y_raw) & (y_raw >= 0)\n",
    "\n",
    "# Apply combined mask\n",
    "X = X[mask_good]\n",
    "y = y_raw[mask_good].astype(int)\n",
    "\n",
    "# Step 3: Inspect\n",
    "print(np.unique(y)[:10])   # print first few unique labels\n",
    "print(y[:20])\n",
    "\n",
    "# Step 4: Safe bincount\n",
    "print(\"Loaded samples:\", X.shape, \"class counts:\", np.bincount(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be49de3",
   "metadata": {},
   "source": [
    "### Train / val / test split, scaling and SMOTE (apply SMOTE only to training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7395a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits -> train: (9090, 138) val: (2273, 138) test: (2841, 138)\n",
      "After SMOTE train distribution: [3040 3040 3040]\n"
     ]
    }
   ],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.2, stratify=y_temp, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(\"Splits -> train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
    "\n",
    "# Standardize using training stats\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# SMOTE on training only\n",
    "smote = SMOTE(random_state=RANDOM_SEED)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE train distribution:\", np.bincount(y_train_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3d412a",
   "metadata": {},
   "source": [
    "### Compute class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43c4e425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# compute class weights from resampled training set (used as alpha for focal loss)\n",
    "cw = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_res), y=y_train_res)\n",
    "class_weights_tensor = torch.tensor(cw, dtype=torch.float32)\n",
    "print(\"Class weights:\", cw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab94d44a",
   "metadata": {},
   "source": [
    "### Compute focal loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95e2ac21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce = F.cross_entropy(inputs, targets, reduction='none', weight=self.alpha)\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf97a5a",
   "metadata": {},
   "source": [
    "### Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2adfa881",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class NODEModel(nn.Module):\n",
    "    def __init__(self, dim, num_classes):\n",
    "        super(NODEModel, self).__init__()\n",
    "        self.odefunc = ODEFunc(dim)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, features) — treat as initial state\n",
    "        t = torch.tensor([0.0, 1.0], dtype=x.dtype, device=x.device)\n",
    "        out = odeint(self.odefunc, x, t)[-1]\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34c8d37",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf92c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b3b079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Simple training function ---\n",
    "def quick_train(model_class, input_dim, num_classes,\n",
    "                train_dataset, val_dataset,\n",
    "                class_weights_tensor,\n",
    "                lr=1e-3,\n",
    "                batch_size=32,\n",
    "                epochs=5,\n",
    "                device=\"cuda\",\n",
    "                save_path=None):\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = model_class(dim=input_dim, num_classes=num_classes).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = FocalLoss(alpha=class_weights_tensor.to(device), gamma=2.0)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                out = model(xb)\n",
    "                _, p = torch.max(out, 1)\n",
    "                preds.extend(p.cpu().numpy())\n",
    "                trues.extend(yb.cpu().numpy())\n",
    "\n",
    "        val_f1 = f1_score(trues, preds, average=\"weighted\")\n",
    "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_f1={val_f1:.4f}\")\n",
    "\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_state = model.state_dict().copy()\n",
    "            if save_path:\n",
    "                torch.save(best_state, save_path)\n",
    "                print(\"  -> New best model saved.\")\n",
    "\n",
    "    print(f\"\\n✅ Training done. Best val F1 = {best_f1:.4f}\")\n",
    "    return model, best_f1, best_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c9f502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.3230, val_f1=0.5762\n",
      "  -> New best model saved.\n",
      "Epoch 2: train_loss=0.3032, val_f1=0.5890\n",
      "  -> New best model saved.\n",
      "Epoch 3: train_loss=0.2916, val_f1=0.6053\n",
      "  -> New best model saved.\n",
      "Epoch 4: train_loss=0.2815, val_f1=0.6150\n",
      "  -> New best model saved.\n",
      "Epoch 5: train_loss=0.2717, val_f1=0.6030\n",
      "\n",
      "✅ Training done. Best val F1 = 0.6150\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_train_res.astype(np.float32)),\n",
    "    torch.from_numpy(y_train_res.astype(np.int64))\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.from_numpy(X_val.astype(np.float32)),\n",
    "    torch.from_numpy(y_val.astype(np.int64))\n",
    ")\n",
    "\n",
    "input_dim = X_train_res.shape[1]\n",
    "num_classes = int(np.unique(y).size)\n",
    "\n",
    "model, best_f1, best_state = quick_train(\n",
    "    model_class=NODEModel,\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    class_weights_tensor=class_weights_tensor,\n",
    "    lr=1e-3,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    device=device,\n",
    "    save_path=OUT_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e6abd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Validation):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5128    0.6053    0.5552       760\n",
      "           1     0.5492    0.3559    0.4319       753\n",
      "           2     0.7613    0.8895    0.8204       760\n",
      "\n",
      "    accuracy                         0.6177      2273\n",
      "   macro avg     0.6078    0.6169    0.6025      2273\n",
      "weighted avg     0.6079    0.6177    0.6030      2273\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Evaluate on validation set and print classification report ---\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "val_preds, val_trues = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in DataLoader(val_dataset, batch_size=32, shuffle=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        outputs = model(xb)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        val_preds.extend(preds.cpu().numpy())\n",
    "        val_trues.extend(yb.cpu().numpy())\n",
    "\n",
    "print(\"\\nClassification Report (Validation):\")\n",
    "print(classification_report(val_trues, val_preds, digits=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a48ed8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 8\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.3237 val_f1: 0.6025\n",
      "  epoch 2 train_loss: 0.3024 val_f1: 0.5817\n",
      "  epoch 3 train_loss: 0.2885 val_f1: 0.6180\n",
      "  epoch 4 train_loss: 0.2793 val_f1: 0.5932\n",
      "  epoch 5 train_loss: 0.2653 val_f1: 0.5766\n",
      "  early stopping\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 16, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.3349 val_f1: 0.5813\n",
      "  epoch 2 train_loss: 0.3156 val_f1: 0.5805\n",
      "  epoch 3 train_loss: 0.3101 val_f1: 0.5947\n",
      "  epoch 4 train_loss: 0.3059 val_f1: 0.5824\n",
      "  epoch 5 train_loss: 0.3022 val_f1: 0.5702\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.3219 val_f1: 0.5840\n",
      "  epoch 2 train_loss: 0.3012 val_f1: 0.6160\n",
      "  epoch 3 train_loss: 0.2882 val_f1: 0.5941\n",
      "  epoch 4 train_loss: 0.2786 val_f1: 0.5986\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.001, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.3462 val_f1: 0.5796\n",
      "  epoch 2 train_loss: 0.3217 val_f1: 0.5751\n",
      "  epoch 3 train_loss: 0.3174 val_f1: 0.5869\n",
      "  epoch 4 train_loss: 0.3143 val_f1: 0.5807\n",
      "  epoch 5 train_loss: 0.3121 val_f1: 0.5957\n",
      "  epoch 6 train_loss: 0.3103 val_f1: 0.5914\n",
      "  epoch 7 train_loss: 0.3080 val_f1: 0.5930\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.3238 val_f1: 0.5679\n",
      "  epoch 2 train_loss: 0.3032 val_f1: 0.5963\n",
      "  epoch 3 train_loss: 0.2918 val_f1: 0.6048\n",
      "  epoch 4 train_loss: 0.2807 val_f1: 0.6028\n",
      "  epoch 5 train_loss: 0.2728 val_f1: 0.6208\n",
      "  epoch 6 train_loss: 0.2614 val_f1: 0.5953\n",
      "  epoch 7 train_loss: 0.2563 val_f1: 0.6092\n",
      "  early stopping\n",
      "  -> New global best. Saved model.\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.3379 val_f1: 0.5913\n",
      "  epoch 2 train_loss: 0.3173 val_f1: 0.5696\n",
      "  epoch 3 train_loss: 0.3138 val_f1: 0.5890\n",
      "  early stopping\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 32, 'optimizer': 'adam'}\n",
      "  epoch 1 train_loss: 0.3255 val_f1: 0.5788\n",
      "  epoch 2 train_loss: 0.3090 val_f1: 0.5754\n",
      "  epoch 3 train_loss: 0.2977 val_f1: 0.5995\n",
      "  epoch 4 train_loss: 0.2865 val_f1: 0.5904\n",
      "  epoch 5 train_loss: 0.2783 val_f1: 0.6017\n",
      "  epoch 6 train_loss: 0.2727 val_f1: 0.6048\n",
      "  epoch 7 train_loss: 0.2656 val_f1: 0.6131\n",
      "  epoch 8 train_loss: 0.2580 val_f1: 0.6105\n",
      "\n",
      "Trying params: {'lr': 0.0005, 'batch_size': 32, 'optimizer': 'sgd'}\n",
      "  epoch 1 train_loss: 0.3476 val_f1: 0.5717\n",
      "  epoch 2 train_loss: 0.3216 val_f1: 0.5653\n",
      "  epoch 3 train_loss: 0.3179 val_f1: 0.5828\n",
      "  epoch 4 train_loss: 0.3157 val_f1: 0.5802\n",
      "  epoch 5 train_loss: 0.3142 val_f1: 0.5938\n",
      "  epoch 6 train_loss: 0.3127 val_f1: 0.5860\n",
      "  epoch 7 train_loss: 0.3112 val_f1: 0.5891\n",
      "  early stopping\n",
      "\n",
      "Global best val F1: 0.6208 params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'adam'}\n",
      "Best params: {'lr': 0.0005, 'batch_size': 16, 'optimizer': 'adam'} Best val F1: 0.6207969194515374\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train_with_hyperparameter_tuning(model_class, input_dim, num_classes,\n",
    "                                     train_dataset, val_dataset,\n",
    "                                     class_weights_tensor,\n",
    "                                     param_grid,\n",
    "                                     save_path=OUT_MODEL,\n",
    "                                     epochs=8,\n",
    "                                     patience=2,\n",
    "                                     device=device):\n",
    "    class_weights_tensor = class_weights_tensor.to(device)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_params = None\n",
    "    best_state = None\n",
    "\n",
    "    combos = list(itertools.product(*param_grid.values()))\n",
    "    print(f\"Total combinations: {len(combos)}\")\n",
    "\n",
    "    for combo in combos:\n",
    "        params = dict(zip(param_grid.keys(), combo))\n",
    "        print(\"\\nTrying params:\", params)\n",
    "\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        model = model_class(dim=input_dim, num_classes=num_classes).to(device)\n",
    "        if params[\"optimizer\"] == \"adam\":\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "        elif params[\"optimizer\"] == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=params[\"lr\"], momentum=0.9)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer\")\n",
    "\n",
    "        criterion = FocalLoss(alpha=class_weights_tensor, gamma=params.get(\"gamma\", 2.0))\n",
    "        epochs_no_improve = 0\n",
    "        local_best = 0.0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            for xb, yb in train_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                out = model(xb)\n",
    "                loss = criterion(out, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item() * xb.size(0)\n",
    "            train_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            preds, trues = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    xb, yb = xb.to(device), yb.to(device)\n",
    "                    out = model(xb)\n",
    "                    _, p = torch.max(out, dim=1)\n",
    "                    preds.extend(p.cpu().numpy()); trues.extend(yb.cpu().numpy())\n",
    "\n",
    "            val_f1 = f1_score(trues, preds, average=\"weighted\")\n",
    "            print(f\"  epoch {epoch} train_loss: {train_loss:.4f} val_f1: {val_f1:.4f}\")\n",
    "\n",
    "            if val_f1 > local_best:\n",
    "                local_best = val_f1\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(\"  early stopping\")\n",
    "                    break\n",
    "\n",
    "        # update global best\n",
    "        if local_best > best_f1:\n",
    "            best_f1 = local_best\n",
    "            best_params = params\n",
    "            best_state = model.state_dict().copy()\n",
    "            torch.save(best_state, save_path)\n",
    "            print(\"  -> New global best. Saved model.\")\n",
    "\n",
    "    print(f\"\\nGlobal best val F1: {best_f1:.4f} params: {best_params}\")\n",
    "    return best_params, best_f1, best_state\n",
    "\n",
    "# Build datasets (reuse existing X_train_res, y_train_res, X_val, y_val)\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train_res.astype(np.float32)), torch.from_numpy(y_train_res.astype(np.int64)))\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_val.astype(np.float32)), torch.from_numpy(y_val.astype(np.int64)))\n",
    "test_dataset = TensorDataset(torch.from_numpy(X_test.astype(np.float32)), torch.from_numpy(y_test.astype(np.int64)))\n",
    "\n",
    "input_dim = X_train_res.shape[1]\n",
    "num_classes = int(np.unique(y).size)\n",
    "\n",
    "param_grid = {\n",
    "    \"lr\": [1e-3, 5e-4],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"optimizer\": [\"adam\", \"sgd\"]\n",
    "}\n",
    "\n",
    "best_params, best_f1, best_state = train_with_hyperparameter_tuning(\n",
    "    model_class=NODEModel,\n",
    "    input_dim=input_dim,\n",
    "    num_classes=num_classes,\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    class_weights_tensor=class_weights_tensor,\n",
    "    param_grid=param_grid,\n",
    "    save_path=OUT_MODEL,\n",
    "    epochs=8,\n",
    "    patience=2,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Best params:\", best_params, \"Best val F1:\", best_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d73c6",
   "metadata": {},
   "source": [
    "### Final evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53412516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test class counts: [950 941 950]\n",
      "Accuracy: 0.6300598380851813\n",
      "F1 (weighted): 0.6314562385528016\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.51      0.52       950\n",
      "           1       0.54      0.57      0.55       941\n",
      "           2       0.84      0.81      0.82       950\n",
      "\n",
      "    accuracy                           0.63      2841\n",
      "   macro avg       0.63      0.63      0.63      2841\n",
      "weighted avg       0.63      0.63      0.63      2841\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      " [[486 380  84]\n",
      " [341 535  65]\n",
      " [104  77 769]]\n"
     ]
    }
   ],
   "source": [
    "# Load best model for final evaluation\n",
    "if best_state is not None:\n",
    "    best_model = NODEModel(dim=input_dim, num_classes=num_classes)\n",
    "    best_model.load_state_dict(best_state)\n",
    "    best_model = best_model.to(device)\n",
    "else:\n",
    "    # fallback: instantiate a fresh model\n",
    "    best_model = NODEModel(dim=input_dim, num_classes=num_classes).to(device)\n",
    "\n",
    "# ensure test_loader exists (create if needed)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Final evaluation\n",
    "best_model.eval()\n",
    "test_preds, test_trues = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        out = best_model(xb)\n",
    "        _, p = torch.max(out, dim=1)\n",
    "        test_preds.extend(p.cpu().numpy())\n",
    "        test_trues.extend(yb.numpy())\n",
    "\n",
    "print(\"Test class counts:\", np.bincount(test_trues))\n",
    "print(\"Accuracy:\", accuracy_score(test_trues, test_preds))\n",
    "print(\"F1 (weighted):\", f1_score(test_trues, test_preds, average=\"weighted\"))\n",
    "print(\"\\nClassification report:\\n\", classification_report(test_trues, test_preds))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(test_trues, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23912680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
